{"id":3446,"date":"2023-10-04T16:31:36","date_gmt":"2023-10-04T20:31:36","guid":{"rendered":"http:\/\/ccvcl.org\/?page_id=3446"},"modified":"2023-11-11T12:12:11","modified_gmt":"2023-11-11T17:12:11","slug":"csc-i6716-fall-2023-assignment-3","status":"publish","type":"page","link":"http:\/\/ccvcl.org\/csc-i6716-fall-2023-assignment-3\/","title":{"rendered":"CSc I6716 Fall 2023 -Assignment 3"},"content":{"rendered":"\n<p>Computer Science \u2013 The City College of New York<br><strong>Computer Vision<br>Assignment 3 ( Deadline:&nbsp;<em>10\/29 Sunday before midnight<\/em>&nbsp; <span class=\"has-inline-color has-vivid-red-color\">&#8211; Extended to Sunday November 5th, 2023<\/span> <span class=\"has-inline-color has-vivid-red-color\">before midnight<\/span>)<\/strong><br>===============================================================<br><strong><em>(Those marked with * are optional for extra credits)<\/em><\/strong><\/p>\n\n\n\n<p>Note:&nbsp; Turn in a PDF document containing a list&nbsp; of your .m files (not the code itself),&nbsp; images showing the results of your experiments such as images, tables, plots, and an analysis of the results. All the writings must be soft copies in print and be sent to Prof. Zhu via email&nbsp;Zhigang Zhu &lt;<a href=\"file:\/\/\/Users\/zzhu\/Documents\/Mac-Pro-Zhu-2014\/Zhigang-Zhu\/Teaching\/VisionCourse\/Spring2018\/Homework\/%3Ccv.zhu.ccny@gmail.com%3E\">cv.zhu.ccny@gmail.com<\/a>&gt;.&nbsp; For the programming part,&nbsp;<strong>send ONLY your source code&nbsp; by email<\/strong>; please don\u2019t send in your images and executable (even if you use C++).&nbsp; You are responsible for the loss of your submissions if you don\u2019t write&nbsp; \u201c<strong>CSC I6716 Computer Vision Assignment 3<\/strong>\u201d in the subject of your email. Do write your name and ID (last four digits) in both both of your report and the code.&nbsp;Please don\u2019t zip your report with your code and other files; send me the report in a separate PDF file. The rest can be in a zipped file.<\/p>\n\n\n\n<hr class=\"wp-block-separator\"\/>\n\n\n\n<p>1&nbsp;&nbsp; (<strong>Camera Models- 20 points<\/strong>)&nbsp; Prove that the vector from the viewpoint of a pinhole camera to the vanishing point (in the image plane) of a set of 3D parallel lines is parallel to the direction of the parallel lines. Please show the steps of your proof.<\/p>\n\n\n\n<p><strong>Hint:<\/strong>&nbsp;You can either use geometric reasoning or algebraic calculation.&nbsp;<\/p>\n\n\n\n<p>If you choose to use geometric reasoning, you can use the fact that the projection of a 3D line in space is the intersection of its \u201cinterpretation plane\u201d with the image plane.&nbsp; Here the interpretation plane (IP) of a 3D line is a plane passing through the 3D line and the center of projection (viewpoint) of the camera.&nbsp; Also, the interpretation planes of two parallel lines intersect in a line passing through the viewpoint, and the intersection line is parallel to the parallel lines.<\/p>\n\n\n\n<p>If you select to use algebraic calculation, you may use the parametric representation of a 3D line: P = P0 +tV, where P= (X,Y,Z)<sup>T<\/sup>&nbsp;is any point on the line (here&nbsp;&nbsp;<sup>T<\/sup>&nbsp;denote for transpose),&nbsp;&nbsp; P0 = (X0,Y0,Z0)<sup>T<\/sup>&nbsp;is a given fixed point on the line, vector V = (a,b,c)<sup>T<\/sup>&nbsp;represents the direction of the line, and t is the scalar parameter that controls the distance (with sign) between P and P0.<\/p>\n\n\n\n<p>If you want to use the determinant formed by three 3D points, you will need to explain details of both the meaning of the determinant, and the steps to arrive your conclusion. Finding a solution somewhere online and copy it in your submission doesn\u2019t work for you.<\/p>\n\n\n\n<p>2. (<strong>Camera Models- 20 points<\/strong>) Show that relation between any image point (xim, yim)<sup>T<\/sup>&nbsp;of a plane (in the form of&nbsp;(x1,x2,x3)<sup>T<\/sup>&nbsp;in projective space ) and its corresponding point (Xw, Yw, Zw)<sup>T<\/sup>&nbsp;on the plane in 3D space can be represented by a 3\u00d73 matrix. You should start from the general form of the camera model (x1,x2,x3)<sup>T<\/sup>&nbsp;= M<sub>int<\/sub>M<sub>ext<\/sub>(Xw, Yw, Zw, 1)<sup>T<\/sup>, where M = M<sub>int<\/sub>M<sub>ext<\/sub>&nbsp;is a 3\u00d74 matrix, with the image center (ox, oy), the focal length f, the scaling factors( sx and sy), &nbsp;the rotation matrix R and the translation vector T all unknown. Note that in the course slides and the lecture notes, I used a simplified model of the perspective project by assuming ox and oy are known and sx = sy =1, and only discussed the special cases of planes.. So you cannot directly copy those equations I used. Nor can you simply derive the 3\u00d74 matrix M.&nbsp; Instead you should use the general form of the projective matrix&nbsp;<strong>(5 points)<\/strong>, and the&nbsp; general form of a plane n<sub>x<\/sub>&nbsp;X<sub>w<\/sub>&nbsp;+ n<sub>y<\/sub>&nbsp;Y<sub>w<\/sub>&nbsp;+ n<sub>z<\/sub>&nbsp;Z<sub>w<\/sub>&nbsp;&nbsp;= d&nbsp;<strong>(5 points)<\/strong>, work on an integration&nbsp;<strong>(5 points),<\/strong>&nbsp;to form a 3\u00d73 matrix between a 3D point on the plane and its 2D image projection&nbsp;<strong>(5 points)<\/strong>.<\/p>\n\n\n\n<p>3.\u00a0\u00a0(<strong>Calibration- 20 points<\/strong>\u00a0)\u00a0 Prove the\u00a0<strong>Orthocenter Theorem<\/strong>\u00a0by geometric arguments:\u00a0<strong>Let T be the triangle on the image plane defined by the three vanishing points of three mutually orthogonal sets of parallel lines in space. Then the image center is the orthocenter of the triangle T (i.e., the common intersection of the three altitudes.\u00a0<\/strong><br>(1)\u00a0\u00a0\u00a0 Basic proof: use the result of Question 1, assuming the aspect ratio of the camera is 1. Note that you are asked to prove the\u00a0Orthcenter Theorem, not just the orthcenter of a triangle\u00a0<strong>(10 points)<\/strong><br>(2)\u00a0\u00a0\u00a0 If you do not know the\u00a0 focal length of the camera, can you still find the image center using the Orthocenter Theorem?\u00a0Explain why or why not\u00a0<strong>(2 points).\u00a0<\/strong>\u00a0Can you also estimate the focal length after you find the image center? If yes, how, and if not, why\u00a0<strong>(3 points)<\/strong><br>(3)\u00a0\u00a0\u00a0 If you do not know the aspect ratio and the focal length of the camera, can you still find the image center using the Orthocenter Theorem <strong>(2 points)<\/strong>?\u00a0Explain why or why not\u00a0<strong>(3 points)<\/strong>.<\/p>\n\n\n\n<p><br>4.&nbsp;<strong>Calibration Programming Exercises (40 points)<\/strong>: Implement the direct parameter calibration method in order to (1) learn how to use SVD to solve systems of linear equations; (2) understand the physical constraints of the camera parameters; and (3) understand important issues related to calibration, such as calibration pattern design, point localization accuracy and robustness of the algorithms. Since calibrating a real camera involves lots of work in calibration pattern design, image processing and error controls as well as solving the equations, we will use simulated data to understand the algorithms.&nbsp;&nbsp;As a by-product we will also learn how to generate 2D images from 3D models using a \u201cvirtual\u201d pinhole camera.<\/p>\n\n\n\n<ul><li><strong>A.Calibration pattern \u201cdesign\u201d.<\/strong>&nbsp;Generate data of a \u201cvirtual\u201d 3D cube similar to the one shown in&nbsp;<a href=\"http:\/\/ccvcl.org\/wp-content\/uploads\/2019\/10\/calibrationpattern.gif\">here<\/a>&nbsp;of the lecture notes in camera calibration. For example, you can hypothesize a 1x1x1 m<sup>3<\/sup>&nbsp;cube and pick up coordinates of 3-D points on one corner of each black square in your world coordinate system. Make sure that the number of your 3-D points is sufficient for the following calibration procedures. In order to show the correctness of your data, draw your cube (with the control points marked) using Matlab (or whatever language you are using). I have provided a piece of&nbsp;<a href=\"http:\/\/www-cs.engr.ccny.cuny.edu\/~zhu\/CSCI6716-2018s\/Homework\/calib.m\">starting code<\/a>&nbsp;in Matlab for you to use.&nbsp;<strong>(5 points)<\/strong><\/li><li><strong>B. \u201cVirtual\u201d camera and images.<\/strong>&nbsp;Design a \u201cvirtual\u201d camera with known intrinsic parameters including focal length f, image center (o<sub>x<\/sub>, o<sub>y<\/sub>) and pixel size (s<sub>x<\/sub>, s<sub>y<\/sub>).&nbsp;&nbsp;As an example, you can assume that the focal length is f = 16 mm, the image frame size is 512*512 (pixels) with an image center (o<sub>x<\/sub>,o<sub>y<\/sub>) = (256, 256), and the size of the image sensor&nbsp;&nbsp;inside your camera is 8.8 mm *6.6 mm (so the pixel size is (s<sub>x<\/sub>,s<sub>y<\/sub>) = (8.8\/512, 6.6\/512) ). Capture an image of your \u201cvirtual\u201d calibration cube with your virtual camera with a given pose (rotation R and translation T).&nbsp;&nbsp;For example, you can take the picture of the cube 4 meters away and with a tilt angle of 30 degree. Use three rotation angles&nbsp;alpha, beta, gamma&nbsp;to generate the rotation matrix R (refer to the lecture notes in camera model \u2013 please double check the equation since it might have typos in signs).&nbsp;&nbsp;You may need to try different poses in order to have a suitable image of your calibration target.&nbsp;<strong>(5 points)<\/strong><\/li><li><strong>C. Direction calibration method:<\/strong>&nbsp;Estimate the intrinsic (f<sub>x<\/sub>, f<sub>y<\/sub>, aspect ratio&nbsp;a, image center (o<sub>x<\/sub>,o<sub>y<\/sub>) ) and extrinsic (R, T and further&nbsp;alpha, beta, gamma) parameters. Use SVD to solve the homogeneous linear system and the least square problem, and to enforce the orthogonality constraint on the estimate of R.&nbsp;<\/li><\/ul>\n\n\n\n<p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0C(i).\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Use the accurately simulated data (both 3D world coordinates and 2D image coordinates) to the algorithms, and compare the results with the \u201cground truth\u201d data (which are given in step (a) and step (b)).\u00a0\u00a0Remember you are practicing a camera calibration, so you should pretend you know nothing about the camera parameters (i.e. you cannot use the ground truth data in your calibration process). However, in the direct calibration method, you could use the knowledge of the image center (in the homogeneous system to find extrinsic parameters) and the aspect ratio (in the Orthocenter theorem method to find image center).\u00a0<strong>\u00a0(20 points)<\/strong><\/p>\n\n\n\n<p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0C(ii).\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Study whether the unknown aspect ratio matters in estimating the image center\u00a0<strong>(5 points)<\/strong>, and how the initial estimation of image center affects the estimating of the remaining parameters\u00a0<strong>(5 points)<\/strong>, by experimental results.\u00a0\u00a0Give a solution to solve the problems if any.<\/p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;C(iii).&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Accuracy Issues. Add in some random noises to the simulated data and run the calibration algorithms again. See how the \u201cdesign tolerance\u201d of the calibration target and the localization errors of 2D image points affect the calibration accuracy. For example, you can add 0.1 mm (or more) random error to 3D points and 0.5 pixel (or more) random error to 2D points. Also analyze how sensitive of the Orthocenter method is to the extrinsic parameters in imaging the three sets of the orthogonal parallel lines.&nbsp;<strong>(* extra points:10<\/strong>)<\/p>\n\n\n\n<p>In all of the steps, you should give you results using either tables or graphs, or both of them.<\/p>\n","protected":false},"excerpt":{"rendered":"<p>Computer Science \u2013 The City College of New YorkComputer VisionAssignment 3 ( Deadline:&nbsp;10\/29 Sunday before midnight&nbsp; &#8211; Extended to Sunday November 5th, 2023 before midnight)===============================================================(Those marked with * are optional&hellip;<\/p>\n","protected":false},"author":3,"featured_media":0,"parent":0,"menu_order":0,"comment_status":"closed","ping_status":"closed","template":"page-templates\/fullwidth.php","meta":{"footnotes":""},"_links":{"self":[{"href":"http:\/\/ccvcl.org\/wp-json\/wp\/v2\/pages\/3446"}],"collection":[{"href":"http:\/\/ccvcl.org\/wp-json\/wp\/v2\/pages"}],"about":[{"href":"http:\/\/ccvcl.org\/wp-json\/wp\/v2\/types\/page"}],"author":[{"embeddable":true,"href":"http:\/\/ccvcl.org\/wp-json\/wp\/v2\/users\/3"}],"replies":[{"embeddable":true,"href":"http:\/\/ccvcl.org\/wp-json\/wp\/v2\/comments?post=3446"}],"version-history":[{"count":6,"href":"http:\/\/ccvcl.org\/wp-json\/wp\/v2\/pages\/3446\/revisions"}],"predecessor-version":[{"id":3536,"href":"http:\/\/ccvcl.org\/wp-json\/wp\/v2\/pages\/3446\/revisions\/3536"}],"wp:attachment":[{"href":"http:\/\/ccvcl.org\/wp-json\/wp\/v2\/media?parent=3446"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}